{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Uber : Determine Best Spots for drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<!-- \n",
    "## <span style=\"color:red\"><b>TODO & Ideas - TO BE COMMENTED</b></span>\n",
    "\n",
    "* Make a test with time_slot_len=6 \n",
    "* fig.write_image(k_AssetsDir + \"/img\"+ \"/K-Means.png\") fonctionne tout simplement pas !!!\n",
    "* Splitting the notebook to avoid size problems with GitHub?\n",
    "* Silhouette graphe en couteau ?\n",
    "\n",
    "* ~~wording : Replace bin by time slot~~\n",
    "* ~~wording : Pickup can be a noun or an adjective, but never a verb. Pick up is a verb phrase, but should not be used as a noun.~~\n",
    "* K-Means\n",
    "    * https://drlee.io/efficient-customer-segmentation-in-retail-using-kmeans-clustering-025c3961bd52\n",
    "* DBSCAN \n",
    "  * ~~metric = haversine car la terre est ronde et pas plate (enfin je crois)~~\n",
    "  * ~~unitÃ© de epsilon~~\n",
    "  * https://blog.stackademic.com/mastering-clustering-dbscan-a880566704bc\n",
    "  * https://www.sefidian.com/2022/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/\n",
    "  \n",
    "* ~~warning K-Means~~\n",
    "  * ~~set OMP_NUM_THREADS=5~~\n",
    "  * ~~Get-ChildItem Env:~~\n",
    "  * ~~echo $env:OMP_NUM_THREADS~~ \n",
    "  \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./assets/ny.png\" alt=\"drawing\" width=\"800\"/>\n",
    "<p>\n",
    "\n",
    "<!-- \n",
    "from IPython.display import display, Image\n",
    "display(Image(filename='/kaggle/input/img-for-uber/ny.png')) \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "* Make sure the drivers are at the right place at the right time such that waiting time for the users does not exceed <span style=\"color:orange\"><b>5 to 7 minutes</b></span> \n",
    "* Ideally an application would recommend hot-zones in major cities to be in at any given time of day\n",
    "* Create an algorithm to find hot zones\n",
    "* Visualize results on a dashboard \n",
    "\n",
    "## Instructions \n",
    "* Get the data from `https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city`\n",
    "* Save it as : ``./assets/archive.zip``\n",
    "* Do <span style=\"color:red\"><b>NOT</b></span> unzip the file\n",
    "* Focus on **New-York**\n",
    "* Focus on April 2014 - September 2014 timeframe\n",
    "* Use cluster coordinates to pin hot zones\n",
    "    * pick-up locations can be gathered into different clusters. Use cluster coordinates to pin hot zones\n",
    "* Create maps with Plotly\n",
    "* Start small then generalize\n",
    "    * Pick one day and a given hour\n",
    "    * **and then** start to generalize your approach\n",
    "\n",
    "## Deliverables\n",
    "* Have a map with hot-zones (Plotly)\n",
    "* At least describe hot-zones per day of week\n",
    "* Compare results with at least : K-Means and DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of concept\n",
    "After reading the instructions, here is what I have in mind :\n",
    "1. The driver opens the app  \n",
    "1. Based on the date and the time of the day  \n",
    "1. A map shows the best spots where the driver should be (black dots in the POC below)  \n",
    "    * As we shall see, the number of spots and their position depend on the date (day of the week) and time of day\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/mock_for_drivers.png\" alt=\"drawing\" width=\"800\"/>\n",
    "<p>\n",
    "\n",
    "<!-- display(Image(filename='/kaggle/input/img-for-uber/mock_for_drivers.png')) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "* [Introduction](#introduction)\n",
    "* [EDA](#eda-exploratory-data-analysis)\n",
    "* [Understanding the dataset](#understanding-the-dataset)\n",
    "* [K-Means and DBSCAN](#identify-clusters-with-k-means-and-dbscan-methods)\n",
    "* [Driver app](#driver-application-mock-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### <span style=\"color:orange\"><b>Comments :</b></span> -->\n",
    "## Setup\n",
    "* I'm an happy Windows 11 user\n",
    "* I also use conda\n",
    "\n",
    "In order to \"play\" on your host with this notebook, follow the steps below ;\n",
    "\n",
    "1. Create a directory\n",
    "1. Get the dataset from `https://www.kaggle.com/datasets/fivethirtyeight/uber-pickups-in-new-york-city`\n",
    "1. Save it as : ``./assets/archive.zip``\n",
    "    * Do <span style=\"color:red\"><b>NOT</b></span> unzip the file\n",
    "1. Save a copy of this notebook in the directory\n",
    "1. Open a terminal in the directory\n",
    "\n",
    "```powershell\n",
    "conda create --name kaggle_uber python=3.13 -y\n",
    "conda activate kaggle_uber\n",
    "conda install numpy pandas seaborn plotly matplotlib scikit-learn nbformat -c conda-forge -y\n",
    "code .\n",
    "```\n",
    "\n",
    "6. Once in VSCode\n",
    "* Comment the line `pio.renderers.default = \"iframe\"` in the prelude below\n",
    "    * It is mandatory in Kaggle only, NOT in VSCOode\n",
    "* In the cell below, comment the line `k_AssetsDir=...` and uncomment the one saying `k_AssetsDir = Path(\"./assets\")`\n",
    "* Search for and comment the line `if False:`\n",
    "* Run this notebook\n",
    "* #### <span style=\"color:red\"><b>ATTENTION :</b></span> \n",
    "    * If you commit this notebook on **GitHub** double check its size **BEFORE** to commit  \n",
    "    * Under VSCode, \"Clear All Outputs\" if needed (I suspect Plotly to keep way too much data)  \n",
    "    * `Get-ChildItem ./ -recurse | where-object {$_.length -gt 100000000} | Sort-Object length | ft fullname, length -auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelude\n",
    "\n",
    "# avoid warnings with K-Means : OpenMP (open multi processing) memory leaks under windows blablabla...\n",
    "import platform\n",
    "system = platform.system()\n",
    "if system == \"Windows\":\n",
    "  import os\n",
    "  os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "else:\n",
    "  None\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import datetime \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder         \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "k_AssetsDir     = Path(\"./assets\")\n",
    "k_Gold          = 1.618              # gold number for ratio\n",
    "k_Width         = 12\n",
    "k_Height        = k_Width/k_Gold\n",
    "k_WidthPx       = 1024\n",
    "k_HeightPx      = k_WidthPx/k_Gold\n",
    "k_random_state  = 0\n",
    "k_Subset_Size   = 50_000             # size of the subset to speedup K-Means computing   \n",
    "k_Time_Slot_Len = 10                 # length, in minutes, of each time slot  \n",
    "k_Months        = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"] # months of interest\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Kaggle only\n",
    "# pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def quick_View(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a summary DataFrame for each column in the input DataFrame.\n",
    "\n",
    "    This function analyzes each column in the given DataFrame and creates a summary that includes\n",
    "    data type, number of null values, percentage of null values, number of non-null values, \n",
    "    number of distinct values, min and max values, outlier bounds (for numeric columns),\n",
    "    and the frequency of distinct values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the summary of each column from the input DataFrame. \n",
    "                      Each row in the resulting DataFrame represents a column from the input DataFrame\n",
    "                      with the following information:\n",
    "                      - \"name\": Column name\n",
    "                      - \"dtype\": Data type of the column\n",
    "                      - \"# null\": Number of null values\n",
    "                      - \"% null\": Percentage of null values\n",
    "                      - \"# NOT null\": Number of non-null values\n",
    "                      - \"distinct val\": Number of distinct values\n",
    "                      - \"-3*sig\": Lower bound for outliers (mean - 3*std) for numeric columns\n",
    "                      - \"min\": Minimum value for numeric columns\n",
    "                      - \"max\": Maximum value for numeric columns\n",
    "                      - \"+3*sig\": Upper bound for outliers (mean + 3*std) for numeric columns\n",
    "                      - \"distinct val count\": Dictionary of distinct value counts or top 10 values for object columns\n",
    "    \"\"\"\n",
    "\n",
    "    summary_lst = []\n",
    "  \n",
    "    for col_name in df.columns:\n",
    "        col_dtype               = df[col_name].dtype\n",
    "        num_of_null             = df[col_name].isnull().sum()\n",
    "        percent_of_null         = num_of_null/len(df)\n",
    "        num_of_non_null         = df[col_name].notnull().sum()\n",
    "        num_of_distinct_values  = df[col_name].nunique()\n",
    "\n",
    "        if num_of_distinct_values <= 10:\n",
    "            distinct_values_counts = df[col_name].value_counts().to_dict()\n",
    "        else:\n",
    "            top_10_values_counts    = df[col_name].value_counts().head(10).to_dict()\n",
    "            distinct_values_counts  = {k: v for k, v in sorted(top_10_values_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        if col_dtype != \"object\":\n",
    "            max_of_col = df[col_name].max()\n",
    "            min_of_col = df[col_name].min()\n",
    "            outlier_hi = df[col_name].mean() + 3*df[col_name].std()\n",
    "            outlier_lo = df[col_name].mean() - 3*df[col_name].std()\n",
    "        else:\n",
    "            max_of_col = -1\n",
    "            min_of_col =  1\n",
    "            outlier_hi = -1\n",
    "            outlier_lo =  1\n",
    "\n",
    "        summary_lst.append({\n",
    "            \"name\"                : col_name,\n",
    "            \"dtype\"               : col_dtype,\n",
    "            \"# null\"              : num_of_null,\n",
    "            \"% null\"              : (100*percent_of_null).round(2),\n",
    "            \"# NOT null\"          : num_of_non_null,\n",
    "            \"distinct val\"        : num_of_distinct_values,\n",
    "            \"-3*sig\"              : round(outlier_lo,2) ,\n",
    "            \"min\"                 : round(min_of_col,2),\n",
    "            \"max\"                 : round(max_of_col,2),\n",
    "            \"+3*sig\"              : round(outlier_hi,2) ,\n",
    "            \"distinct val count\"  : distinct_values_counts\n",
    "        })\n",
    "\n",
    "    df_tmp = pd.DataFrame(summary_lst)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"uber-raw-data-{month}14.csv\"\n",
    "zip_path = k_AssetsDir/\"archive.zip\"\n",
    "output_dir = k_AssetsDir\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    # Iterate over all files in the archive\n",
    "    for file_name in zip_file.namelist():\n",
    "        # Check if the file matches the pattern for any month in the list\n",
    "        if any(file_name == pattern.format(month=month) for month in k_Months):\n",
    "            # Extract the matching file to the output directory\n",
    "            zip_file.extract(file_name, output_dir)\n",
    "            # print(f\"Extracted: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the files available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.read_csv(k_AssetsDir/\"uber-raw-data-apr14.csv\", nrows=5)\n",
    "display (df_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* We cannot use the column named `Base` since we cannot relate it to either a borough or a zone\n",
    "* In files from 2014, we drop the column ``Base`` and only keep ``Date/Time``, ``Lat`` and ``Lon``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Note :</b></span>\n",
    "\n",
    "* Below it is important to notice the feature engineering which is done while the dataset is built\n",
    "* Indeed some columns a extracted from the ``date_time`` features\n",
    "* More important. A ``time_slot`` feature is calculated. More information are given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_preprocessor(df):\n",
    "  df.columns = df.columns.str.lower()\n",
    "  df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "  # df[\"base\"] = df[\"base\"].astype(\"category\")\n",
    "  df.drop(columns=\"base\", inplace=True)\n",
    "\n",
    "  df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])     # dayfirst=True\n",
    "  df[\"year\"] = df[\"date_time\"].dt.year\n",
    "  df[\"month\"] = df[\"date_time\"].dt.month\n",
    "  df[\"day\"] = df[\"date_time\"].dt.day\n",
    "  df[\"weekday\"] = df[\"date_time\"].dt.weekday            # 0 monday, 1 tuesday...\n",
    "  df[\"hour\"] = df[\"date_time\"].dt.hour\n",
    "  df[\"minute\"] = df[\"date_time\"].dt.minute\n",
    "  df.drop(columns=\"date_time\", inplace=True)\n",
    "\n",
    "  # may be we could drop hour and min. We will see how it goes \n",
    "  df[\"time_slot\"] = (df[\"hour\"]*60 + df[\"minute\"])//k_Time_Slot_Len\n",
    "  # df.drop(columns=\"hour\", inplace=True)\n",
    "  # df.drop(columns=\"minute\", inplace=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"apr\"]\n",
    "# months = k_Months            # uncomment/comment this line to take all the months available into account \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for month in months:\n",
    "  df_tmp = pd.read_csv(k_AssetsDir/f\"uber-raw-data-{month}14.csv\")\n",
    "  df = pd.concat([df, df_tmp])\n",
    "\n",
    "df = uber_preprocessor(df)\n",
    "\n",
    "# display(df.sort_values(by=\"time_slot\", ascending=False))\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of duplicates before cleaning : {(df.duplicated().sum()  / len(df)) * 100:.2f}%\")\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "In the dataset :\n",
    "* No more duplicates\n",
    "* A ``time_slot`` is a group of ``k_time_slot_len`` minutes (see the value of ``k_time_slot_len`` in the ``prelude`` cell of this notebook)\n",
    "    * The ``time_slot`` feature is an index indicating the ``time`` at which the observation took place.\n",
    "    * In each day, from 00H00 to 23H59, there are $\\frac{23*60+59}{\\text {k-time-slot-len} }$ ``time_slots`` \n",
    "    * This feature might be useful later since the users are not willing to wait more that 5-7 minutes (the length of a time slot)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on the featured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset consists of :\")\n",
    "print(f\"\\t{len(df.shape):>9_} dimensions\")\n",
    "print(f\"\\t{df.shape[0]:>9_} observations\")\n",
    "print(f\"\\t{df.shape[1]:>9_} features    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types = pd.DataFrame ({\n",
    "  \"types\" : df.dtypes.value_counts()\n",
    "})\n",
    "df_types[\"as_%\"] = (100 * df_types[\"types\"]/df_types[\"types\"].sum()).round(2)\n",
    "\n",
    "display(df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe(include=\"all\").T\n",
    "# df.info()\n",
    "\n",
    "df_tmp = quick_View(df)\n",
    "display(df_tmp.sort_values(by=\"# null\", ascending=False))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* 4.5 M observations when all months are loaded\n",
    "    * 560k observations for april 2014 (see the ``# NOT null`` column)\n",
    "* 0 % of null (see ``% null`` column)\n",
    "* outliers ($\\bar{x}$ + 3 $\\sigma$ ) are in  \n",
    "    * ``lat``\n",
    "    * ``lon``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_outliers = [\"lat\", \"lon\", ]\n",
    "\n",
    "for col in col_outliers:\n",
    "  # fig = px.box(df, y=col)\n",
    "  # fig.show()\n",
    "  # ! Plotly can't handle the whole dataset from april to sept \n",
    "  # Let's go back to an always working safe belt (seaborn) \n",
    "  fig, ax = plt.subplots(figsize=(k_Width, k_Height))\n",
    "  sns.boxplot(df, x=col)\n",
    "  ax.set_title(\"Outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many observations are considered as outliers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_outliers:\n",
    "  upper_bound = df[col].mean() + 3*df[col].std()\n",
    "  lower_bound = df[col].mean() - 3*df[col].std()\n",
    "  nb_out = df.shape[0] - df[((df[col] >= lower_bound) & (df[col] <= upper_bound)) | df[col].isna()].shape[0]\n",
    "  print(f\"{col} have {nb_out:>6_} outliers ({100*nb_out/df.shape[0]:.2} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Since the percentage of outliers is low \n",
    "* In first approximation, we <span style=\"color:red\"><b>remove</b></span> outliers from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def remove_Outliers_Sigma(df, column):\n",
    "    mean_col = df[column].mean()\n",
    "    sigma_col = df[column].std()\n",
    "\n",
    "    lower_bound = mean_col - 3 * sigma_col\n",
    "    upper_bound = mean_col + 3 * sigma_col\n",
    "    df = df[((df[column] >= lower_bound) & (df[column] <= upper_bound)) | df[column].isna()]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before outliers removal : {df.shape}\")\n",
    "for col in col_outliers:\n",
    "    df = remove_Outliers_Sigma(df, col)\n",
    "print(f\"After  outliers removal : {df.shape}\")\n",
    "\n",
    "df_tmp = quick_View(df)\n",
    "display(df_tmp.sort_values(by=\"# null\", ascending=False))                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At this point I consider the EDA is done\n",
    "* However, I need to gain a better understanding of what the data says"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year      = 2014                                                   \n",
    "month     = 4                                                       # select another month if needed (at least, the month must be in df)\n",
    "day       = 9                                                      \n",
    "hour      = 7\n",
    "\n",
    "date_obj = datetime.datetime(year, month, day)                      # day is mandatory to create a date object but it is not yet used\n",
    "date_string = date_obj.strftime(\"%Y-%B\")                            # date_string is used in the title of the graphs\n",
    "\n",
    "# print(date_string)\n",
    "# print(date_obj.strftime('%A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataframe df we extract the selected month of the year of study (see the previous cell) \n",
    "df_full_month  = df[(df[\"month\"] == month) & (df[\"year\"] == year)] \n",
    "\n",
    "# ! Comment the next line if you want to take into account all the pick-up of the month of study  \n",
    "# Plotly may be then slow or unable to display all the data\n",
    "# In addition, some processings may take a while\n",
    "df_month = df_full_month.sample(k_Subset_Size)\n",
    "\n",
    "df_month.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* <span style=\"color:red\"><b>ATTENTION :</b></span> Subset. To help Plotly and speed up the data processing, we use a sample of the pickups of the month of study  \n",
    "* Double check the value of `count` in the table above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickups during the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "  df_month, \n",
    "  x=\"day\",\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"{date_string} - Pickups per day\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* The graph here above shows pickups per day during the month  \n",
    "* It seems there is a kind of cycle along the weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of the pickups per day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "  df_month, \n",
    "  x=\"weekday\", \n",
    "  height = k_HeightPx, \n",
    "  width = k_WidthPx,\n",
    "  title = f\"{date_string} - Pickups per day (0=Monday)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* The weekly pattern is confirmed over the month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "  df,                     # ! df NOT df_month\n",
    "  x=\"weekday\", \n",
    "  height = k_HeightPx, \n",
    "  width = k_WidthPx,\n",
    "  title = \"Apr-Sept 2014 - Pickups per day (0=Monday)\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* The weekly pattern is confirmed over the april-sept period of 2014\n",
    "* Most active days of the week : Wenesday-Friday\n",
    "* Least active days of the week : Sunday-Monday \n",
    "* <span style=\"color:orange\"><b>Consequences :</b></span> \n",
    "    * Since the daily volume pattern is the same whatever the month \n",
    "    * This means that we will have to take the day of the week (monday, tuesday...) into account when looking for hot spots. \n",
    "    * Indeed, if on Wednesday there are twice as many pickups as on Monday, we can assume that the clusters will be of different density and distributed differently.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel density estimate (KDE) represents the data using a continuous probability density curve \n",
    "\n",
    "plt.figure(figsize=(k_Width, k_Height))\n",
    "ax = sns.kdeplot(data = df_month, x = 'hour', fill = True, hue = 'weekday', palette = 'coolwarm', alpha = 0.2) \n",
    "ax.set_title(f\"{date_string} - KDE plots pickups of each day vs hour\")\n",
    "ax.set_ylabel('Density of pickups')\n",
    "ax.set_xticks([i for i in range(25)]) # using bw_adjust= low value ??? and cut=None does'nt work\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* Saturday and Sunday show a pick of activities between 0 and 2AM\n",
    "* Otherwise all the other days show 2 picks around 7AM and 6PM \n",
    "* The density helps to realize why friday (day 4) is so important : not only the peak in the afternoon is high but it is also very large \n",
    "* <span style=\"color:orange\"><b>Consequence :</b></span> \n",
    "    * Since the hourly pattern depend on the day of the week (monday, tuesday...) \n",
    "    * We will have to take it into account when looking for hot spots. \n",
    "    * Indeed, hot spots at 1 AM saturday are, for sure, different than hot spots at 1 AM monday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization of the pickups during the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_month = df_month.sort_values(by= \"day\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_month,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='day', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=3,\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"{date_string} - Localized pickups per day over the selected month\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* The more pickups on one spot, the higher the \"temperature\" of the spot\n",
    "* \"temperature\" obviously refers to the number of pickups in the day\n",
    "* The view is per day along the month of study\n",
    "* At this point there is no information about timing along the day\n",
    "* In April 2014, 21 was a monday. We can \"see\" the previously identified pattern. Indeed 21 (monday) is \"cooler\" than 22 (tuesday) which is \"cooler\" than 23 (wednesday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./assets/hotspots.png\" alt=\"drawing\" width=\"800\"/>\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* No matter the day of the month, the areas here below are always on the top of the list : \n",
    "    * LaGuardia\n",
    "    * Brooklyn\n",
    "    * Greenpoint\n",
    "    * Manhattan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization of the pickups during the day (per hour, no matter the day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_month = df_month.sort_values(by= \"hour\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_month,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='hour', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=3,\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"{date_string} - Localized pickups per hour over the selected month\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* We look at all the days of the month at once\n",
    "* We look how the pickups go per hour during an \"average day\"\n",
    "* Brooklyn and LaGuardia slow down their pickups from 1AM to respectively 4 and 5AM\n",
    "* On the other hand Manhattan and Greenpoint never stop their activites (<span style=\"color:orange\"><b>NY, the city that never sleeps ?</b></span>)\n",
    "* We must understand we <span style=\"color:red\"><b>CANNOT</b></span> use these data to show to the drivers the spots. Indeed :\n",
    "    * All days are considered equal and we know weekend != workday (for example)\n",
    "    * The time resolution is not good enough. The data show were the pickups take place in the next hour while we need n-minute resolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on one day of the week\n",
    "\n",
    "* Monday, tuesday..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the day is the n ith day of the month\n",
    "# Earlier we had\n",
    "# year      = 2014                                                   \n",
    "# month     = 4                                                      \n",
    "# day       = 9                                                      \n",
    "# hour      = 7\n",
    "\n",
    "date_obj = datetime.datetime(year, month, day)\n",
    "date_string = date_obj.strftime(\"%Y-%B-%a\")          # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "# print (date_obj)\n",
    "# print(date_obj.strftime(\"%A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekday = df_month[df_month[\"weekday\"]==date_obj.weekday()]\n",
    "df_weekday.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of the pickups over one weekday (hourly resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(k_Width, k_Height))\n",
    "ax = sns.kdeplot(data = df_weekday, x = 'hour', fill = True, hue = 'weekday', palette = 'coolwarm', alpha = 0.2) \n",
    "ax.set_title(f\"KDE plots pickups of all {date_obj.strftime('%A')} of {date_obj.strftime('%B %Y')} vs hour\")\n",
    "ax.set_ylabel('Density of pickups')\n",
    "ax.set_xticks([i for i in range(25)]) # using bw_adjust= low value ??? and cut=None does'nt work\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* This graph is consistent with the previous KDE plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization of the pickups over one weekday (hourly resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_weekday = df_weekday.sort_values(by= \"hour\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_weekday,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='hour', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=5,\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"Localized pickups for all {date_obj.strftime('%A')} of {date_obj.strftime('%B %Y')} during the next hour\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* Keep in mind that to help Plotly and speed up data processing, we work on a subset of the data.\n",
    "* Bear in mind that the graph shows where the pickups will take place in the next hour for every hour of the day\n",
    "* With some adjustment in zooming and panning, we can observe that at 6AM, the area of Soho (south Manhattan) and east and west side of the middle of Central Park \"lite up\" synchronously\n",
    "* Then the center of Manhattan lites up\n",
    "* Throughout the day, the âfireâ spreads and covers the whole of Manhattan\n",
    "* We can see where the peak of 6 PM takes place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization of the pickups over one weekday (time slot resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_weekday = df_weekday.sort_values(by= \"time_slot\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_weekday,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='time_slot', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=10,                          # ! radius changed from 3 to 10\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"Localized pickups for all {date_obj.strftime('%A')} of {date_obj.strftime('%B %Y')} ({k_Time_Slot_Len} minutes time resolution)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* This is the same graph but with a higher time resolution\n",
    "* Again, keep in mind that \n",
    "    * to help Plotly and speed up data processing, we work on a subset of the data\n",
    "    * the graph shows where the pickups will take place within the next n minutes, for every time slot of the day. For the value of n see the value of ``k_time_slot`` in the prelude cell of this notebook or at the title of the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a try with a ``scatter_mapbox()`` instead of ``density_mapbox()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "  df_weekday, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  # opacity = 0.8,\n",
    "  animation_frame='time_slot', \n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups for all {date_obj.strftime('%A')} of {date_obj.strftime('%B %Y')} ({k_Time_Slot_Len} minutes time resolution)\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do pick-up times and locations differ between Sunday and Monday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sunday  = df_month[(df_month[\"weekday\"] == 6) ] \n",
    "\n",
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_sunday = df_sunday.sort_values(by= \"hour\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_sunday,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='hour', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=3,\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"Localized pickups per hour over all the Sunday of {date_obj.strftime('%B %Y')}\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monday  = df_month[(df_month[\"weekday\"] == 0)] \n",
    "\n",
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_monday = df_monday.sort_values(by= \"hour\", ascending=True)\n",
    "\n",
    "fig = px.density_mapbox(\n",
    "  df_monday,\n",
    "  lat=\"lat\",\n",
    "  lon=\"lon\",\n",
    "  animation_frame='hour', \n",
    "  mapbox_style=\"carto-positron\",\n",
    "  radius=3,\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title= f\"Localized pickups per hour over all Monday of {date_obj.strftime('%B %Y')}\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* At 0 AM we can \"clearly see\" how Sunday differs from Monday (whether people go out at night or not)\n",
    "* Same thing at 6 AM or 7 AM (day off vs working day)\n",
    "* This is consistent with the KDE graph used earlier\n",
    "* <span style=\"color:orange\"><b>Consequence :</b></span> we should understand that the clusters are ``weekday`` and ``hour`` dependant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on one hour from one specific ``weekday``\n",
    "\n",
    "* Since the pattern is `weekday` dependant\n",
    "* We look at the same hour for all the same `weekday` of the same month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_obj = datetime.datetime(year, month, day, hour)\n",
    "date_string = date_obj.strftime(\"%Y-%B-%a-%HH\") # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "print(date_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df_weekday[df_weekday[\"hour\"]==hour]\n",
    "df_hour.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "  df_hour, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups for all {date_obj.strftime('%A')} of {date_obj.strftime('%B %Y')} at {date_obj.strftime('%H')}H during the next hour\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* At this time of the day (7AM) we can see that : \n",
    "    * LaGuardia   \n",
    "    * East and west side of central park\n",
    "    * South Manhattan & Greenpoint\n",
    "\n",
    "are up and running\n",
    "\n",
    "* Brooklyn is waking up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on a n-minute time slot from one specific ``weekday``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_hour = df_hour.sort_values(by= \"time_slot\", ascending=True)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "  df_hour, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  animation_frame='time_slot', \n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups for all {date_obj.strftime('%A')} of the dataset at {date_obj.strftime('%H')}H during the next hour with {k_Time_Slot_Len} minutes resolution\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* The display of the pickups in n-minute batches between 7am and 8am seems difficult to interpret\n",
    "* This might be due to the fact that, in order to help Plotly and speed up data processing, we work on a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on a n-minute time slot from  one specific ``weekday`` of the initial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The idea is to increase the number of observations so\n",
    "    * From the initial dataset...\n",
    "    * Over at most, one year period...\n",
    "    * We select observations from the same weekday at the same hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_same_weekday_same_hour  = df[(df[\"weekday\"] == date_obj.weekday()) & (df[\"year\"] == year) & (df[\"hour\"]==hour)] \n",
    "df_all_same_weekday_same_hour.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is safer to sort the values to be used as an animation_frame \n",
    "df_all_same_weekday_same_hour = df_all_same_weekday_same_hour.sort_values(by= \"time_slot\", ascending=True)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "  df_all_same_weekday_same_hour, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  animation_frame='time_slot', \n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups for all {date_obj.strftime('%A')} at {date_obj.strftime('%H')}H during the next hour with {k_Time_Slot_Len} minutes resolution\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* Compare to the previous graph, we now have 10 times more observations\n",
    "* Even with 10-minute time slots it seems we can \"see\" some clusters\n",
    "* We can groups observations this way because, thanks to the EDA, we now know the pickup are ``weekday`` dependant AND ``time`` dependant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify clusters with `K-Means` and `DBScan` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from where we are:\n",
    "* a specific day of the week (wednesday for example)\n",
    "* an hour (7 AM for example)\n",
    "    \n",
    "We want to identify the areas where a customer is likely to call in the next **10 minutes**.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for the number **k** of clusters so that the clusters are spread out and have a high density. We'll use:\n",
    "\n",
    "* **Elbow Method**:\n",
    "    * We plot WCSS (Within-Cluster Sum of Squares) vs. **k**.\n",
    "    * WCSS is defined as the sum for all cluster of squared distances between the centroids and each point.\n",
    "    * We'll choose **k** such that adding an additional cluster no longer significantly improves the WCSS. This is typically represented as a bend or \"elbow\" in the WCSS curve when plotted against **k**.\n",
    "\n",
    "* **Silhouette Score**:\n",
    "    * The silhouette score measures the quality of the clustering, taking into account both \n",
    "        1. cohesion (how similar the points in the same cluster are) \n",
    "        1. separation (how distinct the clusters are from each other). \n",
    "    * It ranges from -1 to 1 where \n",
    "        1. a value close to 1 indicates that points are well-clustered and clearly separated from other clusters\n",
    "        1. a value near 0 indicates that clusters are close to each other\n",
    "        1. a negative value suggests that points may be incorrectly assigned. \n",
    "    \n",
    "    * The optimal **k** maximizes the silhouette coefficient, meaning the clusters are well-formed (well-separated and compact).\n",
    "\n",
    "* In practice, we select a **k** that is at the \"elbow\" in the WCSS curve and also corresponds to a high silhouette coefficient.\n",
    "* The silhouette coefficient is a measure that evaluates the quality of clustering, taking into account both cohesion (how similar the points in the same cluster are) and separation (how distinct the clusters are from each other).\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage the 7AM is still hardcoded as 42 (42 = 7 * 60/10 + 0 minutes)\n",
    "# First let's make a copy of the features of interest\n",
    "df_for_kmeans = df_all_same_weekday_same_hour[[\"lat\", \"lon\", \"time_slot\"]]\n",
    "# filter and only keep the observations for the next 10 minutes after 7AM\n",
    "df_for_kmeans = df_for_kmeans[df_for_kmeans[\"time_slot\"]==42]\n",
    "df_for_kmeans.drop(columns=\"time_slot\", inplace=True)\n",
    "df_for_kmeans.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "  df_for_kmeans, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups {date_obj.strftime('%A')} at {date_obj.strftime('%H')}H during the next {k_Time_Slot_Len} minutes\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw wcss and slihouette graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "sil=[]\n",
    "k=[]\n",
    "for i in range(2, 20): # Z! 2 to 11\n",
    "  kmeans = KMeans(n_clusters=i, random_state = k_random_state, n_init='auto') # default init='k-means++', n_init='auto' avoid warning\n",
    "  kmeans.fit(df_for_kmeans)\n",
    "  wcss.append(kmeans.inertia_)\n",
    "  sil.append(silhouette_score(df_for_kmeans, kmeans.predict(df_for_kmeans)))\n",
    "  k.append(i)\n",
    "\n",
    "df_for_k_choice = pd.DataFrame({'k':k, 'wcss':wcss, 'sil':sil}).set_index('k')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_for_k_choice.index,\n",
    "        y=df_for_k_choice['wcss'],\n",
    "        mode='lines+markers',\n",
    "        name='WCSS',             # add the name to the legend\n",
    "        line=dict(color='blue')  # color of the line\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_for_k_choice.index,\n",
    "        y=df_for_k_choice['sil'],\n",
    "        name='Silhouette Score',\n",
    "        yaxis='y2',\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    # title_text=f\"{date_string} - KMEANS - Determine optimal # of clusters (k)\",\n",
    "    title_text=f\"Wed-[April-Sept]-2014 - KMEANS - Determine optimal # of clusters (k)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis=dict(\n",
    "        title=\"WCSS\",\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Silhouette Score\",\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    height=k_HeightPx,\n",
    "    width=k_WidthPx,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We choose k that lies at the elbow of the WCSS curve and also corresponds to a high value of the silhouette coefficient. \n",
    "* We select k = 10 \n",
    "* We don't select k=8 because Silhouette score is lower \n",
    "* We don't select k=11 because the WCSS doesn't decrease significantly when k goes from 10 to 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_kmeans_clusters = df_all_same_weekday_same_hour[[\"lat\", \"lon\", \"time_slot\"]]\n",
    "df_with_kmeans_clusters = df_with_kmeans_clusters[df_with_kmeans_clusters[\"time_slot\"]==42]\n",
    "df_with_kmeans_clusters.drop(columns=\"time_slot\", inplace=True)\n",
    "\n",
    "k_optimal = 10\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state = k_random_state, n_init='auto') # default init='k-means++', n_init='auto' avoid warning \n",
    "df_with_kmeans_clusters['cluster'] = kmeans.fit_predict(df_with_kmeans_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display les cluster in colors\n",
    "* The black dots on the map are the centroids of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['lat', 'lon'])\n",
    "\n",
    "# force \"cluster\" to be NOT interpreted as a float number \n",
    "df_with_kmeans_clusters['cluster'] = df_with_kmeans_clusters['cluster'].astype(str)\n",
    "\n",
    "\n",
    "# df_with_kmeans_clusters = df_with_kmeans_clusters.sort_values(by= \"time_slot\", ascending=True)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "  df_with_kmeans_clusters, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "#   animation_frame='time_slot', \n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  color='cluster',\n",
    "  color_discrete_sequence=px.colors.qualitative.Plotly,                             # Use a discrete color sequence : Plotly D3 Set1 Pastel1 Category10\n",
    "  # color_discrete_sequence=['red', 'blue', 'green', 'purple', 'orange', 'black'],  # Custom color sequence\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"K-Means - Localized pickups {date_obj.strftime('%A')} at {date_obj.strftime('%H')}H within the next {k_Time_Slot_Len} minutes\"\n",
    ")\n",
    "\n",
    "# fig.add_trace(px.scatter_mapbox(\n",
    "#     kmeans_centroids,\n",
    "#     lat='lat',\n",
    "#     lon='lon',\n",
    "#     mapbox_style=\"carto-positron\"\n",
    "# ).data[0])\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=kmeans_centroids['lat'],\n",
    "    lon=kmeans_centroids['lon'],\n",
    "    mode='markers',\n",
    "    marker=go.scattermapbox.Marker(\n",
    "        size=10,           \n",
    "        color='black',       \n",
    "        opacity=0.8        \n",
    "    ),\n",
    "    name='Centroids'       \n",
    "))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Simply does NOT work\n",
    "# conda install -c conda-forge python-kaleido\n",
    "# fig.write_image(k_AssetsDir + \"/img\"+ \"/K-Means.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "THE thing to understand is :\n",
    "* Even if a cluster is very large (cluster 0, Manhattan for example)\n",
    "* If the cab is in the cluster\n",
    "* Then the density of the cluster (in terms of customers who will call) is such that the cab will be able to serve a customer in less than ``k_time_slot`` min.\n",
    "* Example \n",
    "    * Large Manhattan cluster\n",
    "    * The cab is on the south side\n",
    "    * There may be a call from a customer on the north side of Manhattan that the cab won't be able to serve in less than 5 min...\n",
    "    * <span style=\"color:orange\"><b>BUT</b></span>, we are also sure that there will be a call from a customer in the SOUTH of Manhattan that the driver will be able to serve in less than 5 min.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "\n",
    "K-Means can be used to determine from which clusters a customer is likely to call in the next 10 minutes.\n",
    "\n",
    "So we could imagine that the driver presses a button of an on an application. From that moment on, the application could :\n",
    "1. finds the day of the week (`weekday`)\n",
    "1. finds the time\n",
    "1. invoque ``kmean.fit()`` \n",
    "\n",
    "However, this is where we reach one of the <span style=\"color:red\"><b>limitations</b></span> of K-Means. Indeed, one have to <span style=\"color:red\"><b>choose k</b></span> before the application can go on invoking ``kmeans.fit_predict()`` and display the clusters on a map. Yes we could automate one way or another the way to find optimal k. For example, this could be done using the slope of WCSS and checking how the Silhouette score evolve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage the 7AM is still hardcoded as 42 (42 = 7 * 60/10 + 0 minutes)\n",
    "# First let's make a copy of the features of interrest\n",
    "df_for_dbscan = df_all_same_weekday_same_hour[[\"lat\", \"lon\", \"time_slot\"]]\n",
    "# filter and only keep the observations for the next 10 minutes after 7AM\n",
    "df_for_dbscan = df_for_dbscan[df_for_dbscan[\"time_slot\"]==42]\n",
    "df_for_dbscan.drop(columns=\"time_slot\", inplace=True)\n",
    "df_for_dbscan.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "  df_for_dbscan, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  mapbox_style = \"carto-positron\",  \n",
    "#   color='cluster',\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"Localized pickups {date_obj.strftime('%A')} at {date_obj.strftime('%H')}H within the next {k_Time_Slot_Len} minutes\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! Conversion radian\n",
    "df_for_dbscan_rad = np.deg2rad(df_for_dbscan)\n",
    "df_for_dbscan_rad.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_dbscan = np.deg2rad(df_for_dbscan[[\"lat\", \"lon\"]])\n",
    "# df_for_dbscan.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg radius of Earth in kilometers\n",
    "# kms_per_radian = 6371.0088         \n",
    "\n",
    "# pickups within 3 km\n",
    "# epsilon = 3000/ kms_per_radian\n",
    "\n",
    "epsilon = 0.0001 # in radian because lat and lon have been converted to radian (0.05 dÂ° => 0.00087 rad)\n",
    "                 # 0.0001 rad = 637 m\n",
    "min_samples = 4  # heuristic 2 * number_of_dimensions => 2*2=4\n",
    "\n",
    "\n",
    "# ! metric='haversine' => https://blog.stackademic.com/mastering-clustering-dbscan-a880566704bc\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples, metric='haversine', algorithm=\"auto\")       # algorithm='ball_tree', metric=\"euclidean\"\n",
    "dbscan.fit(df_for_dbscan_rad)                                                                     # Y a pas de predict\n",
    "\n",
    "# df_hour_extended = df_hour.copy()\n",
    "df_for_dbscan['cluster'] = dbscan.labels_\n",
    "\n",
    "# df_tmp = pd.DataFrame(db.labels_)\n",
    "# df_tmp[0].value_counts()\n",
    "\n",
    "print(set(dbscan.labels_))\n",
    "print(len(set(dbscan.labels_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force \"cluster\" to be NOT interpreted as a float number \n",
    "df_for_dbscan['cluster'] = df_for_dbscan['cluster'].astype(str)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "  df_for_dbscan[df_for_dbscan['cluster']!=\"-1\"],  # do not display anomalies\n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  color='cluster',\n",
    "  color_discrete_sequence=px.colors.qualitative.Plotly,                             # Use a discrete color sequence : Plotly D3 Set1 Pastel1 Category10\n",
    "  # color_discrete_sequence=['red', 'blue', 'green', 'purple', 'orange', 'black'],  # Custom color sequence\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"DBSCAN - Localized pickups {date_obj.strftime('%A')} at {date_obj.strftime('%H')}H within the next {k_Time_Slot_Len} minutes\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select epsilon ?\n",
    "# https://www.sefidian.com/2022/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=4)             # heuristic 2 * number_of_dimensions => 2*2=4\n",
    "nbrs = neigh.fit(df_for_dbscan_rad)\n",
    "distances, indices = nbrs.kneighbors(df_for_dbscan_rad) # distance and indices of the 4 nearest neighbors\n",
    "# print(distances.shape)\n",
    "# print(distances)\n",
    "\n",
    "# Sort distances by ascending order\n",
    "distances = np.sort(distances[:, 3])                   # extract & sort the 4th nearest neighbor\n",
    "# print(distances.shape)\n",
    "# print(distances)\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Points index')\n",
    "plt.ylabel('Distance to 4th nearest neighbor')\n",
    "plt.title('k-distance Graph')\n",
    "\n",
    "plt.ylim(0, 0.0005)\n",
    "plt.xlim(600, 850)                                   # \"zoom\" in the region of interest\n",
    "\n",
    "# xv and yh are \"hand adjusted\" to cross at tje \"elbow\" nicely\n",
    "xv = 765\n",
    "yh = 0.00012                                         # epsilon\n",
    "plt.axvline(x=xv, color='red', linestyle='--')\n",
    "plt.axhline(y=yh, color='red', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could we use the derivative to automate the detection of the previous elbow ?\n",
    "# print(distances)\n",
    "derivative = [distances[i + 1] - distances[i] for i in range(len(distances) - 1)]\n",
    "# print(differences)\n",
    "\n",
    "plt.plot(derivative)\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Distance to 4th nearest neighbor')\n",
    "plt.title('Derivative of the k-distance Graph')\n",
    "\n",
    "plt.ylim(0, 0.0002)\n",
    "plt.xlim(600, 850)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver application mock-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the mock-up this is where the driver enters the day of the week and the time\n",
    "# Yes, the application could determine automatically the weekday and the time\n",
    "\n",
    "year    = 2014\n",
    "weekday = 0\n",
    "time    = \"07:30\"\n",
    "\n",
    "# weekday = 2\n",
    "# time = \"07:00\"\n",
    "\n",
    "# weekday = 0\n",
    "# time = \"01:00\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]            \n",
    "week_days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "day_of_week = week_days[weekday]\n",
    "hour, minute = map(float, time.split(\":\"))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for month in months:\n",
    "  df_tmp = pd.read_csv(f\"assets/uber-raw-data-{month}14.csv\")\n",
    "  df = pd.concat([df, df_tmp])\n",
    "\n",
    "df = uber_preprocessor(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "col_outliers = [\"lat\", \"lon\"]\n",
    "for col in col_outliers:\n",
    "    df = remove_Outliers_Sigma(df, col)\n",
    "\n",
    "df_tmp = quick_View(df)\n",
    "display(df_tmp.sort_values(by=\"# null\", ascending=False))                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_same_weekday_same_hour  = df[(df[\"weekday\"] == weekday) & (df[\"year\"] == year) & (df[\"hour\"]==hour) & (df[\"minute\"]==minute)] \n",
    "# df_all_same_weekday_same_hour  = df[(df[\"weekday\"] == weekday) & (df[\"year\"] == year) & (df[\"hour\"]==hour)] \n",
    "# df_all_same_weekday_same_hour.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_kmeans = df_all_same_weekday_same_hour[[\"lat\", \"lon\", \"time_slot\"]]\n",
    "# filter and only keep the observations for the next 10 minutes after hour\n",
    "df_for_kmeans = df_for_kmeans[df_for_kmeans[\"time_slot\"]==(hour*60+minute)/k_Time_Slot_Len]\n",
    "df_for_kmeans.drop(columns=\"time_slot\", inplace=True)\n",
    "# df_for_kmeans.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "sil=[]\n",
    "k=[]\n",
    "for i in range(2, 20): # Z! 2 to 11\n",
    "  kmeans = KMeans(n_clusters=i, random_state = k_random_state, n_init='auto') # default init='k-means++', n_init='auto' avoid warning\n",
    "  kmeans.fit(df_for_kmeans)\n",
    "  wcss.append(kmeans.inertia_)\n",
    "  sil.append(silhouette_score(df_for_kmeans, kmeans.predict(df_for_kmeans)))\n",
    "  k.append(i)\n",
    "\n",
    "df_for_k_choice = pd.DataFrame({'k':k, 'wcss':wcss, 'sil':sil}).set_index('k')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_for_k_choice.index,\n",
    "        y=df_for_k_choice['wcss'],\n",
    "        mode='lines+markers',\n",
    "        name='WCSS',             # Ajout du nom dans la lÃ©gende\n",
    "        line=dict(color='blue')  # couleur pour la ligne\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_for_k_choice.index,\n",
    "        y=df_for_k_choice['sil'],\n",
    "        name='Silhouette Score',\n",
    "        yaxis='y2',\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"KMEANS - Determine optimal # of clusters (k)\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis=dict(\n",
    "        title=\"WCSS\",\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Silhouette Score\",\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    height=k_HeightPx,\n",
    "    width=k_WidthPx,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, k_optimal will be determined automatically\n",
    "# See the next cell for example\n",
    "# As today you can either use 10 (seems to be working most of the time) or adjust it base on what you see on the previous graph\n",
    "\n",
    "k_optimal = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! Prototype\n",
    "def select_optimal_k(wcss, silhouette_scores, k_values):\n",
    "    # Elbow method: Select k where the WCSS curve bends\n",
    "    # Choose k where WCSS decreases significantly slower (i.e., the \"elbow\")\n",
    "    wcss_diffs = np.diff(wcss)                                                      # first derivative\n",
    "    wcss_diffs_diffs = np.diff(wcss_diffs)                                          # second derivative \n",
    "    optimal_k = k_values[np.argmin(wcss_diffs_diffs) + 1]\n",
    "    # print(f\"k via elbow :{optimal_k}\")\n",
    "    \n",
    "    # Silhouette method \n",
    "    # After k is selected via elbow, look around k +/- 1 (or 2..., see delta) and then pick k that maximizes the silhouette score within the window\n",
    "    delta = 1\n",
    "    lower_bound = max(0, optimal_k - delta - min(k_values))                         # make sure don't go below or above k_values range\n",
    "    upper_bound = min(len(k_values) - 1, optimal_k + delta - min(k_values))  \n",
    "    subset_k_values = k_values[lower_bound:upper_bound + 1]\n",
    "    # print(subset_k_values)\n",
    "    subset_silhouette_scores = silhouette_scores[lower_bound:upper_bound + 1]\n",
    "    # print(subset_silhouette_scores)\n",
    "    optimal_k = subset_k_values[np.argmax(subset_silhouette_scores)]\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "optimal_k = select_optimal_k(df_for_k_choice[\"wcss\"], df_for_k_choice[\"sil\"], df_for_k_choice.index)\n",
    "print(f\"Optimal k : {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_kmeans_clusters = df_all_same_weekday_same_hour[[\"lat\", \"lon\", \"time_slot\"]]\n",
    "df_with_kmeans_clusters = df_with_kmeans_clusters[df_with_kmeans_clusters[\"time_slot\"]==(hour*60+minute)/k_Time_Slot_Len]\n",
    "df_with_kmeans_clusters.drop(columns=\"time_slot\", inplace=True)\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state = k_random_state, n_init='auto') # default init='k-means++', n_init='auto' avoid warning \n",
    "df_with_kmeans_clusters['cluster'] = kmeans.fit_predict(df_with_kmeans_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['lat', 'lon'])\n",
    "\n",
    "# force \"cluster\" to be NOT interpreted as a float number \n",
    "df_with_kmeans_clusters['cluster'] = df_with_kmeans_clusters['cluster'].astype(str)\n",
    "\n",
    "\n",
    "# df_with_kmeans_clusters = df_with_kmeans_clusters.sort_values(by= \"time_slot\", ascending=True)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "  df_with_kmeans_clusters, \n",
    "  lat=\"lat\", \n",
    "  lon=\"lon\", \n",
    "  opacity = 0.5,\n",
    "#   animation_frame='time_slot', \n",
    "  mapbox_style = \"carto-positron\",  \n",
    "  color='cluster',\n",
    "  color_discrete_sequence=px.colors.qualitative.Plotly,                             # Use a discrete color sequence : Plotly D3 Set1 Pastel1 Category10\n",
    "  # color_discrete_sequence=['red', 'blue', 'green', 'purple', 'orange', 'black'],  # Custom color sequence\n",
    "  zoom = 10.0,\n",
    "  height = k_HeightPx,\n",
    "  width = k_WidthPx,\n",
    "  title = f\"K-Means - Localized pickups {day_of_week} at {int(hour):02d}:{int(minute):02d} within the next {k_Time_Slot_Len} minutes\"\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=kmeans_centroids['lat'],\n",
    "    lon=kmeans_centroids['lon'],\n",
    "    mode='markers',\n",
    "    marker=go.scattermapbox.Marker(\n",
    "        size=10,           \n",
    "        color='black',       \n",
    "        opacity=0.8        \n",
    "    ),\n",
    "    name='Centroids'       \n",
    "))\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_uber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
